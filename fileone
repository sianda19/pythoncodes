
import requests
from bs4 import BeautifulSoup
import time

# Print the paragraphs
import spacy

# Load English language model
nlp = spacy.load("en_core_web_sm")

def contains_crime(text):
    # Process the text using spaCy
    doc = nlp(text)
    
    # Define a list of crime-related keywords
    crime_keywords = ["crime", "criminal", "theft", "robbery", "assault", "murder", "fraud", "illegal", "violence"]
  
    # Check if any token in the document matches a crime keyword
    for token in doc:
        if token.text.lower() in crime_keywords:
            return True
    return False

# Define your paragraphs

key_paragraphs = []

def scrape_paragraphs_from_multiple_urls(urls):
    all_paragraphs = []
    # List of User-Agent strings to rotate
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
        # Add more User-Agent strings as needed
    ]
    headers = {'User-Agent': ''}



    for url in urls:
        try:
            print(url)
            # Rotate User-Agent
            headers['User-Agent'] = user_agents[len(all_paragraphs) % len(user_agents)]
            
            # Fetch the HTML content of the web page
            response = requests.get(url, headers=headers)
            response.raise_for_status()  # Raise an exception for bad requests

            # Decode the content with the specified encoding, ignoring errors
            content = response.content.decode('utf-8', errors='ignore')

            # Parse the HTML using BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')

            # Find all paragraph elements
            paragraphs = soup.find_all('p')

            # Extract text from paragraph elements and add them to the list
            for paragraph in paragraphs:
                paragraph_text = paragraph.get_text()
                all_paragraphs.append(paragraph_text)

            # Introduce a delay to avoid bombarding the server
            time.sleep(1)  # Sleep for 1 second between requests
        except Exception as e:
            continue

         #  scrape_paragraphs_from_multiple_urls(urls)
          # print(f"An error occurred while processing {url}: {e}")

    #return all_paragraphs
    for i, paragraph in enumerate(all_paragraphs, start=1):
        strings = str(paragraph)
      # if "Delf" in strings:
        key_paragraphs.append(strings)
        if contains_crime(strings):
          print(strings)
          decide()
          print(f"Paragraph {i} contains mentions of crime.")
        else:
          print(strings)
          print(f"Paragraph {i} does not contain mentions of crime.")
      
 
# Example usage
urls = []

print(key_paragraphs)



from googlesearch import search

def get_google_links(query, num_results=10,country="ZA"):
    try:
        # Perform the Google search
        search_results = search(query)
        
        # Return the links
        return search_results
    except Exception as e:
        print("An error occurred:", e)
        return []

# Example usage
query ="is Gugulethu cape town safe"
num_results = 5  # Number of results you want to retrieve
links = get_google_links(query, num_results)
count = 0
for i, link in enumerate(links, start=1):
    count += 1
    if count < 10:
     urls.append(link)
     print(f"{i}. {link}")
    else:
        
        break

#print(urls)
scrape_paragraphs_from_multiple_urls(urls)



def decide():
  






